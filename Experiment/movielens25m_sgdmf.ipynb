{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tensorflow model for Dictionary Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Library')\n",
    "\n",
    "from Metrics.RMSE import RMSE\n",
    "from Metrics.CrossTab import CrossTab\n",
    "from Modules.SGDMF import SGDMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Movie Lens 25M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "dataset = pd.read_csv('../Data/MovieLens/Recommended/dataset.csv')\n",
    "\n",
    "# Display the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of ratings\n",
    "rating_counts = dataset['Rating'].value_counts().sort_index()\n",
    "plt.bar(rating_counts.index.astype(str), rating_counts.values)\n",
    "plt.title('Distribution of ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AdvancedModelSelection import user_based_train_val_test_split\n",
    "\n",
    "# Split the dataset into train, validation, and test data\n",
    "train_data, val_data, test_data = user_based_train_val_test_split(dataset, test_size=0.2, val_size=0.1, random_state=42)\n",
    "\n",
    "# Print the shapes of train and test data\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MOVIES = dataset['Movie Index'].max() + 1\n",
    "NUM_USERS = dataset['User Index'].max() + 1\n",
    "NUM_FACTORS = 32\n",
    "BATCH_SIZE = 32\n",
    "ALPHA = 0.1\n",
    "BETA = 0.0\n",
    "INIT_SCALE = dataset['Rating'].mean() / NUM_FACTORS\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data to sparse tensor\n",
    "train_sparse_tensor = tf.sparse.SparseTensor(\n",
    "    indices=train_data[['Movie Index', 'User Index']].values,\n",
    "    values=train_data['Rating'].values,\n",
    "    dense_shape=[NUM_MOVIES, NUM_USERS]\n",
    ")\n",
    "train_sparse_tensor = tf.sparse.reorder(train_sparse_tensor)\n",
    "\n",
    "# Convert validation data to sparse tensor\n",
    "val_sparse_tensor = tf.sparse.SparseTensor(\n",
    "    indices=val_data[['Movie Index', 'User Index']].values,\n",
    "    values=val_data['Rating'].values,\n",
    "    dense_shape=[NUM_MOVIES, NUM_USERS]\n",
    ")\n",
    "val_sparse_tensor = tf.sparse.reorder(val_sparse_tensor)\n",
    "\n",
    "# Convert test data to sparse tensor\n",
    "test_sparse_tensor = tf.sparse.SparseTensor(\n",
    "    indices=test_data[['Movie Index', 'User Index']].values,\n",
    "    values=test_data['Rating'].values,\n",
    "    dense_shape=[NUM_MOVIES, NUM_USERS]\n",
    ")\n",
    "test_sparse_tensor = tf.sparse.reorder(test_sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "def create_train_dataset(batch_size=BATCH_SIZE):\n",
    "    def data_generator():\n",
    "        train_slices = tf.sparse.split(sp_input=train_sparse_tensor, num_split=NUM_USERS // batch_size, axis=1)\n",
    "        val_slices = tf.sparse.split(sp_input=val_sparse_tensor, num_split=NUM_USERS // batch_size, axis=1)\n",
    "        for i in range(NUM_USERS // batch_size):\n",
    "            yield (tf.sparse.to_dense(train_slices[i]), tf.sparse.to_dense(val_slices[i]))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        data_generator, \n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=[NUM_MOVIES, None], dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=[NUM_MOVIES, None], dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_full_dataset(batch_size=BATCH_SIZE):\n",
    "    def data_generator():\n",
    "        train_slices = tf.sparse.split(sp_input=train_sparse_tensor, num_split=NUM_USERS // batch_size, axis=1)\n",
    "        val_slices = tf.sparse.split(sp_input=val_sparse_tensor, num_split=NUM_USERS // batch_size, axis=1)\n",
    "        test_slices = tf.sparse.split(sp_input=test_sparse_tensor, num_split=NUM_USERS // batch_size, axis=1)\n",
    "        for i in range(NUM_USERS // batch_size):\n",
    "            yield (tf.sparse.to_dense(train_slices[i]), tf.sparse.to_dense(val_slices[i]), tf.sparse.to_dense(test_slices[i]))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        data_generator, \n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=[NUM_MOVIES, None], dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=[NUM_MOVIES, None], dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=[NUM_MOVIES, None], dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "### 1. Batch size vs train time vs RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "batch_sizes = [8, 32, 128, 512]\n",
    "rmse_train_results = np.zeros((len(batch_sizes), 5))\n",
    "rmse_val_results = np.zeros((len(batch_sizes), 5))\n",
    "train_timer = np.zeros((len(batch_sizes)))\n",
    "\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "\n",
    "    # Create the model\n",
    "    model = SGDMF(NUM_MOVIES, NUM_FACTORS, INIT_SCALE, ALPHA, BETA)\n",
    "    rmse_train = RMSE()\n",
    "    rmse_val = RMSE()\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = create_train_dataset(batch_size)\n",
    "        \n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(5):\n",
    "        progress_bar = tqdm(total=NUM_USERS // batch_size, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "        \n",
    "        for train_batch, val_batch in dataset:\n",
    "            x = model(train_batch)\n",
    "            rmse_train.update_state(train_batch, tf.matmul(model.C, x), tf.cast(tf.not_equal(train_batch, 0.0), tf.float32))\n",
    "            rmse_val.update_state(val_batch, tf.matmul(model.C, x), tf.cast(tf.not_equal(val_batch, 0.0), tf.float32))\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "\n",
    "        print(\"Train RMSE:\", rmse_train.result().numpy())\n",
    "        print(\"Val RMSE:\", rmse_val.result().numpy())\n",
    "        rmse_train_results[i, epoch] = rmse_train.result().numpy()\n",
    "        rmse_val_results[i, epoch] = rmse_val.result().numpy()\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Calculate the elapsed time\n",
    "    train_timer[i] = elapsed_time  # Store the elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'Batch Size': batch_sizes,\n",
    "    'Train RMSE': rmse_train_results[:, -1],\n",
    "    'Validation RMSE': rmse_val_results[:, -1],\n",
    "    'Time (s)': train_timer\n",
    "})\n",
    "\n",
    "results.to_csv('Results/ML25M/SGDMF/batch_sizes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Number of factors vs RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "num_factors = [1, 2, 4, 8, 12, 16, 24, 32, 48, 64]\n",
    "rmse_train_results = np.zeros((len(num_factors), 5))\n",
    "rmse_val_results = np.zeros((len(num_factors), 5))\n",
    "train_timer = np.zeros((len(num_factors)))\n",
    "\n",
    "for i, num_factor in enumerate(num_factors):\n",
    "\n",
    "    # Create the model\n",
    "    model = SGDMF(NUM_MOVIES, num_factor, INIT_SCALE, ALPHA, BETA)\n",
    "    rmse_train = RMSE()\n",
    "    rmse_val = RMSE()\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = create_train_dataset()\n",
    "        \n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(5):\n",
    "        progress_bar = tqdm(total=NUM_USERS // BATCH_SIZE, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "        \n",
    "        for train_batch, val_batch in dataset:\n",
    "            x = model(train_batch)\n",
    "            rmse_train.update_state(train_batch, tf.matmul(model.C, x), tf.cast(tf.not_equal(train_batch, 0.0), tf.float32))\n",
    "            rmse_val.update_state(val_batch, tf.matmul(model.C, x), tf.cast(tf.not_equal(val_batch, 0.0), tf.float32))\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "\n",
    "        print(\"Train RMSE:\", rmse_train.result().numpy())\n",
    "        print(\"Val RMSE:\", rmse_val.result().numpy())\n",
    "        rmse_train_results[i, epoch] = rmse_train.result().numpy()\n",
    "        rmse_val_results[i, epoch] = rmse_val.result().numpy()\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Calculate the elapsed time\n",
    "    train_timer[i] = elapsed_time  # Store the elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'Number of Factors': num_factors,\n",
    "    'Train RMSE': rmse_train_results[:, -1],\n",
    "    'Validation RMSE': rmse_val_results[:, -1],\n",
    "    'Time (s)': train_timer\n",
    "})\n",
    "\n",
    "results.to_csv('Results/ML25M/SGDMF/num_factors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Number of epochs vs RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "rmse_train_results = np.zeros(NUM_EPOCHS)\n",
    "rmse_val_results = np.zeros(NUM_EPOCHS)\n",
    "rmse_test_results = np.zeros(NUM_EPOCHS)\n",
    "\n",
    "# Create the model\n",
    "model = SGDMF(NUM_MOVIES, NUM_FACTORS, INIT_SCALE, ALPHA, BETA)\n",
    "rmse_train = RMSE()\n",
    "rmse_val = RMSE()\n",
    "rmse_test = RMSE()\n",
    "crosstab_test = CrossTab(len(rating_counts.index))\n",
    "\n",
    "# Create the dataset\n",
    "dataset = create_full_dataset()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    progress_bar = tqdm(total=NUM_USERS // BATCH_SIZE, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    \n",
    "    for train_batch, val_batch, test_batch in dataset:\n",
    "        x = None\n",
    "        \n",
    "        if epoch == 0:\n",
    "            x, v_updates = model(train_batch, return_V_updates=True)\n",
    "            pd.DataFrame(v_updates.numpy()).to_csv('Results/ML25M/SGDMF/V_updates.csv', index=False, header=False, mode='a')\n",
    "        else:\n",
    "            x = model(train_batch)\n",
    "        \n",
    "        rmse_train.update_state(train_batch, tf.matmul(model.C, x), tf.cast(tf.not_equal(train_batch, 0.0), tf.float32))\n",
    "        rmse_val.update_state(val_batch, tf.matmul(model.C, x), tf.cast(tf.not_equal(val_batch, 0.0), tf.float32))\n",
    "        rmse_test.update_state(test_batch, tf.matmul(model.C, x), tf.cast(tf.not_equal(test_batch, 0.0), tf.float32))\n",
    "        \n",
    "        if epoch == NUM_EPOCHS - 1:\n",
    "            test_mask = tf.cast(tf.not_equal(test_batch, 0.0), tf.int32)\n",
    "            test_batch = tf.cast((test_batch * 2.0) - 1.0, tf.int32)\n",
    "            pred_batch = tf.cast(tf.round((tf.clip_by_value(tf.matmul(model.C, x), 1.0, 5.0) * 2.0) - 1.0), tf.int32)\n",
    "            crosstab_test.update_state(test_batch, pred_batch, test_mask)\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "\n",
    "    print(\"Train RMSE:\", rmse_train.result().numpy())\n",
    "    print(\"Val RMSE:\", rmse_val.result().numpy())\n",
    "    print(\"Test RMSE:\", rmse_test.result().numpy())\n",
    "    rmse_train_results[epoch] = rmse_train.result().numpy()\n",
    "    rmse_val_results[epoch] = rmse_val.result().numpy()\n",
    "    rmse_test_results[epoch] = rmse_test.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'Train Results': rmse_train_results,\n",
    "    'Validation Results': rmse_val_results,\n",
    "    'Test Results': rmse_test_results\n",
    "})\n",
    "results.to_csv('Results/ML25M/SGDMF/num_epochs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the confusion matrix to a CSV file\n",
    "crosstab = crosstab_test.result().numpy().astype(int)\n",
    "crosstab_df = pd.DataFrame(crosstab, index=rating_counts.index, columns=rating_counts.index)\n",
    "crosstab_df.to_csv('Results/ML25M/SGDMF/confusion_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "model_path = \"Results/ML25M/SGDMF/model_weights\"\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.write(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variables in the model\n",
    "tf.train.list_variables(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "model_path = \"Results/ML25M/model_weights\"\n",
    "model = SGDMF(NUM_MOVIES, NUM_FACTORS, INIT_SCALE, ALPHA, BETA)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the first test batch\n",
    "dataset = create_full_dataset()\n",
    "train_batch, val_batch, test_batch = next(iter(dataset))\n",
    "x = model(train_batch)\n",
    "pred = tf.matmul(model.C, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predictions\n",
    "for i, j in tf.where(tf.not_equal(test_batch, 0.0)):\n",
    "    print(f\"i: {i.numpy()}, j: {j.numpy()}, True: {test_batch[i, j].numpy()}, Pred: {pred[i, j].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the results from the CSV file\n",
    "results = pd.read_csv('Results/ML25M/SGDMF/batch_sizes.csv')\n",
    "\n",
    "batch_sizes = results['Batch Size']\n",
    "train_rmse = results['Train RMSE']\n",
    "val_rmse = results['Validation RMSE']\n",
    "train_time = results['Time (s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time taken to train the model vs batch size\n",
    "plt.plot(batch_sizes, train_time, marker='o')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Time taken to train the model vs Batch Size')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE of the last epoch vs batch size\n",
    "plt.plot(batch_sizes, train_rmse, marker='o', label='Train')\n",
    "plt.plot(batch_sizes, val_rmse, marker='o', label='Validation')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Batch Size')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the results from the CSV file\n",
    "results = pd.read_csv('Results/ML25M/SGDMF/num_factors.csv')\n",
    "\n",
    "num_factors = results['Number of Factors']\n",
    "train_rmse = results['Train RMSE']\n",
    "val_rmse = results['Validation RMSE']\n",
    "time_taken = results['Time (s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time taken to train the model vs number of factors\n",
    "plt.plot(num_factors, time_taken, marker='o')\n",
    "plt.xlabel('Number of Factors')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Time taken to train the model vs Number of Factors')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE of the last epoch vs number of factors\n",
    "plt.plot(num_factors, train_rmse, marker='o', label='Train')\n",
    "plt.plot(num_factors, val_rmse, marker='o', label='Validation')\n",
    "plt.xlabel('Number of Factors')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Number of Factors')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the results from the CSV file\n",
    "results = pd.read_csv('Results/ML25M/SGDMF/num_epochs.csv')\n",
    "\n",
    "rmse_train_results = results['Train Results']\n",
    "rmse_val_results = results['Validation Results']\n",
    "rmse_test_results = results['Test Results']\n",
    "\n",
    "# Plot RMSE vs number of epochs\n",
    "plt.plot(np.arange(1, NUM_EPOCHS+1), rmse_train_results, marker='.', label='Train')\n",
    "plt.plot(np.arange(1, NUM_EPOCHS+1), rmse_val_results, marker='.', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the V updates\n",
    "v_updates = pd.read_csv('Results/ML25M/SGDMF/V_updates.csv', header=None)\n",
    "\n",
    "# Plot the V updates\n",
    "for i in range(len(v_updates.columns)):\n",
    "    plt.plot(v_updates[i][:10000], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('$\\\\frac{V_{k-1}}{\\lambda + x_k^\\intercal V_{k-1} x_k}$', fontsize=14)\n",
    "plt.title('Updates of V')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the confusion matrix from the CSV file\n",
    "crosstab_df = pd.read_csv('Results/ML25M/SGDMF/confusion_matrix.csv', index_col=0)\n",
    "crosstab = crosstab_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "sns.heatmap(crosstab, annot=True, fmt=\"d\", cmap='Blues', cbar=False)\n",
    "plt.xticks(np.arange(0.5, 0.5 + len(rating_counts.index)), rating_counts.index.astype(str))\n",
    "plt.yticks(np.arange(0.5, 0.5 + len(rating_counts.index)), rating_counts.index.astype(str))\n",
    "plt.ylabel('Actual Rating')\n",
    "plt.xlabel('Predicted Rating')\n",
    "plt.title('Confusion Matrix of actual vs predicted ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
